{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests: Presidential Contributions\n",
    "\n",
    "Let's look at a random forests models for the presidential dataset.\n",
    "\n",
    "This dataset defines all presidential contribution amounts from publicly available information.\n",
    "\n",
    "The purpose here is to try to classify the candidate to whom the contributor contributes.  \n",
    "\n",
    "Here are the feature columns we will use:\n",
    "1. Last Name (converted from Contributor Name)\n",
    "2. First Name (converted from Contributor Name)\n",
    "3. State \n",
    "4. Latitude (converted from Zipcode)\n",
    "5. Longitude (converted from zipcode)\n",
    "6. Employer\n",
    "7. Occupation\n",
    "\n",
    "### Notes\n",
    "\n",
    "This is going to be a very difficult dataset to get high accuracy, because we don't have any features that are highly correlated with the outcome. Part of our analysis is to see which features prove to be the most useful. \n",
    "\n",
    "One might suspect that information like State, might be very predictive -- because presumably New Yorkers might contribute to Hillary Clinton and Texans might contribute to Donald Trump. However, it turns out that State is pretty weakly correlated to the outcome.  \n",
    "\n",
    "One nice thing about random forests is that since we \"bag\" featues in differnet trees, we can empirically see which variables have hte most predictive power.  This is helpful for analytical reasons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation  import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/data/presidential_election_contribs/2016/2016-medium-clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_column = ['CAND_NM']\n",
    "numeric_columns = ['LAT', 'LNG']\n",
    "feature_columns = ['LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'LAT', 'LNG', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n",
    "categorical_columns = ['LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Print out a count broken down by candidate? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out a grouping by candidate\n",
    "# TODO: Print Breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> Which candidates got the most donations? (in terms of number of donors) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Make Feature Vector, and Index Categorical Columns\n",
    "\n",
    "Let's build a feature Vector\n",
    "\n",
    "Let's index all the categorical columns, and build a labeld index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_v = dataset[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO replace all categorical columns in feature_v by calling pd.factorize on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Build vectors from all the numeric and categorical_index columns **\n",
    "\n",
    "**HINT: numeric_columns and categorical_index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Random Forest Model\n",
    "\n",
    "create the model here.\n",
    "\n",
    "**=> TODO: Create Random Forest Model **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train a RandomForest model.\n",
    "rf2 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split data into training and test\n",
    "\n",
    "**=> TODO: build training and test datasets 70%/30% **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "\n",
    "trainingData2, testData2 = train_test_split(features_scaled,  test_size=???, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n",
    "\n",
    "**=> TODO: Get predictions by transforming testData2 **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "\n",
    "\n",
    "model2 = pipeline2.fit(trainingData2)\n",
    "\n",
    "# Make predictions.\n",
    "predictions2 = model2.transform(???)  #Transform test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaulate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate our accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Think about the test error here?  Does it seem high?  What does that say about our model?**\n",
    "\n",
    "**=> How do we define model success? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the Label\n",
    "\n",
    "0. Hillary Clinton\n",
    "1. Bernie Sanders\n",
    "2. Donald Trump\n",
    "3. Ted Cruz\n",
    "\n",
    "## Step 7: Print out the Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Print out a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the list above to interpret the label.  \n",
    "\n",
    "**=>What can you conclude from the confusion matrix?**\n",
    "\n",
    "Is our model better at predicting candidates with many donations (Clinton, Sanders), or few donations?\n",
    "\n",
    "What can you say about our model perfromance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Print the feature importanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the feautre importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(numeric_columns + categorical_columns) # for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO Compare the relative weight of the feature importances? **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Most important Fields\n",
    "\n",
    "1. Employer\n",
    "2. Occupation\n",
    "3. LastName\n",
    "4. State\n",
    "\n",
    "Other fields not significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO Compare the relative weight of the feature importances? **\n",
    "\n",
    "Why do you think that the lat/long and other fields did not contribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> BONUS: Do a Pearson Correlation Matrix of the variables to the outcome, to see correlation **\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
